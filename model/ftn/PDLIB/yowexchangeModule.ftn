!> Has only the ghost nodes assign to a neighbor domain
module yowExchangeModule
  use yowDatapool, only: rkind
  use MPI, only: MPI_DATATYPE_NULL
  implicit none
  private
  public :: initNbrDomains, createMPITypes, setDimSize
  public :: finalizeExchangeModule, PDLIB_exchange1Dreal
  public :: PDLIB_exchange1Dint, PDLIB_exchange2Dreal, PDLIB_exchange3Dreal

  !> Holds some data belong to a neighbor Domain
  type, public :: t_neighborDomain
    
    !> the domain ID
    !> The domain ID of the neighbor domain. Starts by 1
    integer :: domainID = 0

    !> number of ghosts nodes.
    !> holds the number of ghosts nodes the two domains share together
    !> (the neighbor domain has a copy of the ghosts. when you change some
    !> value in a ghost node, it dosen't change in the node on the other domain)
    integer :: numNodesToReceive = 0

    !> this are the ghosts that we.
    !> has in this neighbor domain. global node IDs
    integer, allocatable :: nodesToReceive(:)

    !> number of nodes we have to send to this neighbor
    integer :: numNodesToSend = 0

    !> this are the ghosts from this neighbor.
    !> global node IDs to send
    integer, allocatable :: nodesToSend(:)

    !> MPI datatypes for 1D exchange
    integer :: p1DRsendType = MPI_DATATYPE_NULL
    integer :: p1DRrecvType = MPI_DATATYPE_NULL
    integer :: p1DIsendType = MPI_DATATYPE_NULL
    integer :: p1DIrecvType = MPI_DATATYPE_NULL
    !> MPI datatypes for 2D exchange
    integer :: p2DRsendType1 = MPI_DATATYPE_NULL
    integer :: p2DRrecvType1 = MPI_DATATYPE_NULL
    integer :: p2DRsendType2 = MPI_DATATYPE_NULL
    integer :: p2DRrecvType2 = MPI_DATATYPE_NULL
    !> MPI datatypes for 3D exchange
    integer :: p3DRsendType = MPI_DATATYPE_NULL
    integer :: p3DRrecvType = MPI_DATATYPE_NULL

    contains
!     procedure :: exchangeGhostIds
!     final :: finalizeNeighborDomain
    procedure :: finalize
    procedure :: createMPIType

  end type

  !> Knows for all domains neighbors, which node we must send or revc from neighbor domains
  !> from 1 to nConnDomains
  type(t_neighborDomain), public, allocatable :: neighborDomains(:)

  !> Number of neighbor domains
  integer, public :: nConnDomains = 0

  !> number of the second dimension for exchange
  integer, public :: n2ndDim = 1

  !> number fo the third dimension for exchange
  integer, public :: n3ndDim = 1

  contains


  subroutine finalize(this)
    use yowerr
    use MPI
    implicit none
    class(t_neighborDomain), intent(inout) :: this
    integer :: ierr

    if(allocated(this%nodesToSend))    deallocate(this%nodesToSend)
    if(allocated(this%nodesToReceive)) deallocate(this%nodesToReceive)

    call mpi_type_free(this%p1DRsendType, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
    call mpi_type_free(this%p1DRrecvType, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
    call mpi_type_free(this%p1DIsendType, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
    call mpi_type_free(this%p1DIrecvType, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
    call mpi_type_free(this%p2DRsendType1, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
    call mpi_type_free(this%p2DRrecvType1, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
    call mpi_type_free(this%p2DRsendType2, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
    call mpi_type_free(this%p2DRrecvType2, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
    call mpi_type_free(this%p3DRsendType, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
    call mpi_type_free(this%p3DRrecvType, ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("freeMPItype", ierr)
  end subroutine

  ! create MPI indexed datatype for this neighborDomain
  subroutine createMPIType(this)
    use yowerr
    use MPI
    use yowNodepool, only: ghostgl, np, ipgl
    use yowDatapool, only: rtype, itype
    implicit none
    class(t_neighborDomain), intent(inout) :: this

    integer :: ierr
    integer :: dsplSend(this%numNodesToSend)
    integer :: dsplRecv(this%numNodesToReceive)

    
    dsplSend = ipgl(this%nodesToSend)-1
    dsplRecv = ghostgl(this%nodesToReceive) + np -1

    ! p1D real
    call mpi_type_create_indexed_block(this%numNodesToSend, 1, dsplSend, rtype, this%p1DRsendType,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
    call mpi_type_commit(this%p1DRsendType,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
    
    call mpi_type_create_indexed_block(this%numNodesToReceive, 1, dsplRecv, rtype, this%p1DRrecvType,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
    call mpi_type_commit(this%p1DRrecvType,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)

    ! p1D integer
    call mpi_type_create_indexed_block(this%numNodesToSend, 1, dsplSend, itype, this%p1DIsendType,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
    call mpi_type_commit(this%p1DIsendType,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)

    call mpi_type_create_indexed_block(this%numNodesToReceive, 1, dsplRecv, itype, this%p1DIrecvType,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
    call mpi_type_commit(this%p1DIrecvType,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)

    ! p2D real
    dsplSend = (ipgl(this%nodesToSend)-1) * n2ndDim
    dsplRecv = (ghostgl(this%nodesToReceive) + np -1) * n2ndDim
    call mpi_type_create_indexed_block(this%numNodesToSend, n2ndDim, dsplSend, rtype, this%p2DRsendType1,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
    call mpi_type_commit(this%p2DRsendType1,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)

    call mpi_type_create_indexed_block(this%numNodesToReceive, n2ndDim, dsplRecv, rtype, this%p2DRrecvType1,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
    call mpi_type_commit(this%p2DRrecvType1,ierr)
    if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)


    IF (n3ndDim .gt. 0) THEN
      dsplSend = (ipgl(this%nodesToSend)-1) * n3ndDim
      dsplRecv = (ghostgl(this%nodesToReceive) + np -1) * n3ndDim
      call mpi_type_create_indexed_block(this%numNodesToSend, n3ndDim, dsplSend, rtype, this%p2DRsendType2,ierr)
      if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
      call mpi_type_commit(this%p2DRsendType2,ierr)
      if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)

      call mpi_type_create_indexed_block(this%numNodesToReceive, n3ndDim, dsplRecv, rtype, this%p2DRrecvType2,ierr)
      if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
      call mpi_type_commit(this%p2DRrecvType2,ierr)
      if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)

      ! p3D real
      dsplSend = (ipgl(this%nodesToSend)-1) * n2ndDim*n3ndDim
      dsplRecv = (ghostgl(this%nodesToReceive) + np -1) * n2ndDim*n3ndDim
      call mpi_type_create_indexed_block(this%numNodesToSend, n2ndDim*n3ndDim, dsplSend, rtype, this%p3DRsendType,ierr)
      if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
      call mpi_type_commit(this%p3DRsendType,ierr)
      if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)

      call mpi_type_create_indexed_block(this%numNodesToReceive, n2ndDim*n3ndDim, dsplRecv, rtype, this%p3DRrecvType,ierr)
      if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
      call mpi_type_commit(this%p3DRrecvType,ierr)
      if(ierr /= MPI_SUCCESS) CALL PARALLEL_ABORT("createMPIType", ierr)
    END IF
  end subroutine

  subroutine initNbrDomains(nConnD)
    use yowerr
    implicit none
    integer, intent(in) :: nConnD
    integer :: stat

    call finalizeExchangeModule()
    nConnDomains = nConnD
    allocate(neighborDomains(nConnDomains), stat=stat)
    if(stat/=0)  CALL ABORT('neighborDomains allocation failure')
  end subroutine

  subroutine createMPITypes()    
    implicit none
    integer :: i

    do i=1, nConnDomains
      call neighborDomains(i)%createMPIType()
    end do
  end subroutine

  !> exchange values in U.
  !> \param[inout] U array with values to exchange. np+ng long.
  !> Send values from U(1:np) to other threads.
  !> Receive values from other threads and updates U(np+1:np+ng
  !> \note MPI recv tag: 10000 + MPI rank
  !> \note MPI send tag: 10000 + neighbor MPI rank
  subroutine PDLIB_exchange1Dreal(U)
    use yowDatapool, only: comm, myrank, rkind
    use yowNodepool, only: t_Node, nodes_global, np, ng, ghosts, npa
    use yowerr
    use MPI
    implicit none
    real(kind=rkind), intent(inout) :: U(:)

    integer :: i, ierr, tag
    integer :: sendRqst(nConnDomains), recvRqst(nConnDomains)
    integer :: recvStat(MPI_STATUS_SIZE, nConnDomains), sendStat(MPI_STATUS_SIZE, nConnDomains)
    character(len=140) :: errmsg

    if(size(U) /= npa) then
      WRITE(errmsg, *) 'size(U)=', size(U), ' but npa=', npa
      CALL ABORT(errmsg)
    endif

    ! post receives
    do i=1, nConnDomains
      tag = 10000 + myrank
      call MPI_IRecv(U, 1, neighborDomains(i)%p1DRrecvType, &
          neighborDomains(i)%domainID-1, tag, comm, &
          recvRqst(i), ierr)
      if(ierr/=MPI_SUCCESS) then
        CALL PARALLEL_ABORT("MPI_IRecv", ierr)
      endif
    enddo

    ! post sends
    do i=1, nConnDomains
      tag = 10000 + (neighborDomains(i)%domainID-1)
      call MPI_ISend(U, 1, neighborDomains(i)%p1DRsendType, &
          neighborDomains(i)%domainID-1, tag, comm, &
          sendRqst(i), ierr);
      if(ierr/=MPI_SUCCESS) then
        CALL PARALLEL_ABORT("MPI_ISend", ierr)
      endif
    end do

    ! Wait for completion
    call mpi_waitall(nConnDomains, recvRqst, recvStat,ierr)
    if(ierr/=MPI_SUCCESS) CALL PARALLEL_ABORT("waitall", ierr)
    call mpi_waitall(nConnDomains, sendRqst, sendStat,ierr)
    if(ierr/=MPI_SUCCESS) CALL PARALLEL_ABORT("waitall", ierr)
  end subroutine

  !> \overload exchange1Dreal
  !> \note MPI recv tag: 20000 + MPI rank
  !> \note MPI send tag: 20000 + neighbor MPI rank
  subroutine PDLIB_exchange1Dint(U)
    use yowDatapool, only: comm, myrank
    use yowNodepool, only: t_Node, nodes_global, np, ng, ghosts, npa
    use yowerr
    use MPI
    implicit none
    integer, intent(inout) :: U(:)

    integer :: i, ierr, tag
    integer :: sendRqst(nConnDomains), recvRqst(nConnDomains)
    integer :: recvStat(MPI_STATUS_SIZE, nConnDomains), sendStat(MPI_STATUS_SIZE, nConnDomains)

    if(size(U) /= npa) then
      CALL ABORT("sizeof(U) < npa")
    endif

    ! post receives
    do i=1, nConnDomains
      tag = 20000 + myrank
      call MPI_IRecv(U, 1, neighborDomains(i)%p1DIrecvType, &
             neighborDomains(i)%domainID-1, tag, comm, &
             recvRqst(i), ierr)
      if(ierr/=MPI_SUCCESS) then
        CALL PARALLEL_ABORT("MPI_IRecv", ierr)
      endif
    enddo

    ! post sends
    do i=1, nConnDomains
      tag = 20000 + (neighborDomains(i)%domainID-1)
      call MPI_ISend(U, 1, neighborDomains(i)%p1DIsendType, &
             neighborDomains(i)%domainID-1, tag, comm, &
             sendRqst(i), ierr)
      if(ierr/=MPI_SUCCESS) then
        CALL PARALLEL_ABORT("MPI_ISend", ierr)
      endif
    end do

    ! Wait for completion
    call mpi_waitall(nConnDomains, recvRqst, recvStat,ierr)
    if(ierr/=MPI_SUCCESS) CALL PARALLEL_ABORT("waitall", ierr)
    call mpi_waitall(nConnDomains, sendRqst, sendStat,ierr)
    if(ierr/=MPI_SUCCESS) CALL PARALLEL_ABORT("waitall", ierr)
  end subroutine

  !> \overload PDLIB_exchange1Dreal
  !> 
  !> \note MPI recv tag: 30000 + MPI rank
  !> \note MPI send tag: 30000 + neighbor MPI rank
  subroutine PDLIB_exchange2Dreal(U)
    use yowDatapool, only: comm, myrank, rkind
    use yowNodepool, only: t_Node, nodes_global, np, ng, ghosts, npa
    use yowerr
    use MPI
    USE W3ODATMD, only : IAPROC
    implicit none
    real(kind=rkind), intent(inout) :: U(:,:)

    integer :: i, ierr, tag
    integer :: sendRqst(nConnDomains), recvRqst(nConnDomains)
    integer :: recvStat(MPI_STATUS_SIZE, nConnDomains), sendStat(MPI_STATUS_SIZE, nConnDomains)


!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 1'
!/DEBUGEXCH     FLUSH(740+IAPROC)

    if(size(U,2) /= npa) then
      CALL ABORT("sizeof(U,2) < npa")
    endif
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 2'
!/DEBUGEXCH     FLUSH(740+IAPROC)

    if((size(U,1) /= n2ndDim) ) then
      if((size(U,1) /= n3ndDim) ) then
        CALL ABORT("sizeof(U,1) /= n2ndDim or n3ndDim")
      endif
    endif
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 3'
!/DEBUGEXCH     FLUSH(740+IAPROC)

    !> \todo do that better
    if(size(U,1) == n2ndDim) then
      ! post receives
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 4'
!/DEBUGEXCH     FLUSH(740+IAPROC)
      do i=1, nConnDomains
        tag = 30000 + myrank
        call MPI_IRecv(U, 1, neighborDomains(i)%p2DRrecvType1, &
               neighborDomains(i)%domainID-1, tag, comm, &
               recvRqst(i), ierr)
        if(ierr/=MPI_SUCCESS) then
          CALL PARALLEL_ABORT("MPI_IRecv", ierr)
        endif
      enddo
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 5'
!/DEBUGEXCH     FLUSH(740+IAPROC)

      ! post sends
      do i=1, nConnDomains
        tag = 30000 + (neighborDomains(i)%domainID-1)
        call MPI_ISend(U, 1, neighborDomains(i)%p2DRsendType1, &
               neighborDomains(i)%domainID-1, tag, comm, &
               sendRqst(i), ierr)
        if(ierr/=MPI_SUCCESS) then
          CALL PARALLEL_ABORT("MPI_ISend", ierr)
        endif
      end do
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 6'
!/DEBUGEXCH     FLUSH(740+IAPROC)
    else if(size(U,1) == n3ndDim) then
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 7'
!/DEBUGEXCH     FLUSH(740+IAPROC)
      ! post receives
      do i=1, nConnDomains
        tag = 30000 + myrank
        call MPI_IRecv(U, 1, neighborDomains(i)%p2DRrecvType2, &
               neighborDomains(i)%domainID-1, tag, comm, &
               recvRqst(i), ierr)
        if(ierr/=MPI_SUCCESS) then
          CALL PARALLEL_ABORT("MPI_IRecv", ierr)
        endif
      enddo
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 8'
!/DEBUGEXCH     FLUSH(740+IAPROC)

      ! post sends
      do i=1, nConnDomains
        tag = 30000 + (neighborDomains(i)%domainID-1)
        call MPI_ISend(U, 1, neighborDomains(i)%p2DRsendType2, &
            neighborDomains(i)%domainID-1, tag, comm, &
            sendRqst(i), ierr);
        if(ierr/=MPI_SUCCESS) then
          CALL PARALLEL_ABORT("MPI_ISend", ierr)
        endif
      end do
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 9'
!/DEBUGEXCH     FLUSH(740+IAPROC)
    endif
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 10'
!/DEBUGEXCH     FLUSH(740+IAPROC)

    ! Wait for completion
    call mpi_waitall(nConnDomains, recvRqst, recvStat,ierr)
    if(ierr/=MPI_SUCCESS) CALL PARALLEL_ABORT("waitall", ierr)
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 11'
!/DEBUGEXCH     FLUSH(740+IAPROC)
    call mpi_waitall(nConnDomains, sendRqst, sendStat,ierr)
    if(ierr/=MPI_SUCCESS) CALL PARALLEL_ABORT("waitall", ierr)
!/DEBUGEXCH     WRITE(740+IAPROC,*) 'PDLIB_exchange2Dreal, step 12'
!/DEBUGEXCH     FLUSH(740+IAPROC)
  end subroutine

  !> \overload exchange1Dreal
  !> \note MPI recv tag: 40000 + MPI rank
  !> \note MPI send tag: 40000 + neighbor MPI rank
  subroutine PDLIB_exchange3Dreal(U)
    use yowDatapool, only: comm, myrank, rkind
    use yowNodepool, only: t_Node, nodes_global, np, ng, ghosts, npa
    use yowerr
    use MPI
    implicit none
    real(kind=rkind), intent(inout) :: U(:,:,:)

    integer :: i, ierr, tag
    integer :: sendRqst(nConnDomains), recvRqst(nConnDomains)
    integer :: recvStat(MPI_STATUS_SIZE, nConnDomains), sendStat(MPI_STATUS_SIZE, nConnDomains)


    if(size(U,3) /= npa) then
      CALL ABORT("sizeof(U,3) < npa")
    endif

    if((size(U,2) /= n2ndDim) ) then
      CALL ABORT("sizeof(U,2) < n2ndDim")
    endif

    if((size(U,1) /= n3ndDim) ) then
      CALL ABORT("sizeof(U,1) < n3ndDim")
    endif

    ! post receives
    do i=1, nConnDomains
      tag = 40000 + myrank
      call MPI_IRecv(U, 1, neighborDomains(i)%p3DRrecvType, &
          neighborDomains(i)%domainID-1, tag, comm, &
          recvRqst(i), ierr)
      if(ierr/=MPI_SUCCESS) then
        CALL PARALLEL_ABORT("MPI_IRecv", ierr)
      endif
    enddo

    ! post sends
    do i=1, nConnDomains
      tag = 40000 + (neighborDomains(i)%domainID-1)
      call MPI_ISend(U, 1, neighborDomains(i)%p3DRsendType, &
          neighborDomains(i)%domainID-1, tag, comm, &
          sendRqst(i), ierr)
      if(ierr/=MPI_SUCCESS) then
        CALL PARALLEL_ABORT("MPI_ISend", ierr)
      endif
    end do

    ! Wait for completion
    call mpi_waitall(nConnDomains, recvRqst, recvStat,ierr)
    if(ierr/=MPI_SUCCESS) CALL PARALLEL_ABORT("waitall", ierr)
    call mpi_waitall(nConnDomains, sendRqst, sendStat,ierr)
    if(ierr/=MPI_SUCCESS) CALL PARALLEL_ABORT("waitall", ierr)
  end subroutine

  !> set the size of the second and third dimension for exchange
  !> \note the size of the first dimension is npa
  !> \note call this before initPD()
  subroutine setDimSize(second, third)
    implicit none
    integer, intent(in) :: second, third
    n2ndDim = second
    n3ndDim = third
  end subroutine setDimSize

  subroutine finalizeExchangeModule()
    implicit none
    integer :: i

    if(allocated(neighborDomains)) then
      do i=1, size(neighborDomains)
        call neighborDomains(i)%finalize()
      end do
      deallocate(neighborDomains)
    endif
  end subroutine
end module yowExchangeModule
